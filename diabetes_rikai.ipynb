{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c1f3b8c-de9c-4745-9084-5801e0a6aa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42dea63-bd4a-4a31-8a2b-7b75092446d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/da/.pyenv/versions/3.8.10/envs/rikai-example/lib/python3.8/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/da/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/da/.ivy2/jars\n",
      "ai.eto#rikai_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ad187b6d-01d1-409a-aec4-5f924e2f33db;1.0\n",
      "\tconfs: [default]\n",
      "\tfound ai.eto#rikai_2.12;0.0.11 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8-1 in local-m2-cache\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in spark-list\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      "\tfound io.circe#circe-core_2.12;0.12.3 in central\n",
      "\tfound io.circe#circe-numbers_2.12;0.12.3 in central\n",
      "\tfound org.typelevel#cats-core_2.12;2.0.0 in central\n",
      "\tfound org.typelevel#cats-macros_2.12;2.0.0 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.0.0 in central\n",
      "\tfound io.circe#circe-generic_2.12;0.12.3 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.3 in spark-list\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in spark-list\n",
      "\tfound io.circe#circe-parser_2.12;0.12.3 in central\n",
      "\tfound io.circe#circe-jawn_2.12;0.12.3 in central\n",
      "\tfound org.typelevel#jawn-parser_2.12;0.14.2 in central\n",
      "\tfound org.apache.logging.log4j#log4j-core;2.13.0 in central\n",
      ":: resolution report :: resolve 4320ms :: artifacts dl 21ms\n",
      "\t:: modules in use:\n",
      "\tai.eto#rikai_2.12;0.0.11 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.3 from spark-list in [default]\n",
      "\tio.circe#circe-core_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-generic_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-jawn_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-numbers_2.12;0.12.3 from central in [default]\n",
      "\tio.circe#circe-parser_2.12;0.12.3 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8-1 from local-m2-cache in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-core;2.13.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from spark-list in [default]\n",
      "\torg.typelevel#cats-core_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#cats-macros_2.12;2.0.0 from central in [default]\n",
      "\torg.typelevel#jawn-parser_2.12;0.14.2 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from spark-list in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   18  |   1   |   1   |   0   ||   18  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ad187b6d-01d1-409a-aec4-5f924e2f33db\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 18 already retrieved (0kB/13ms)\n",
      "21/10/14 11:36:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2021-10-14 11:37:11,782 INFO Rikai (callback_service.py:54): Spark callback server started\n",
      "2021-10-14 11:37:11,789 INFO Rikai (callback_service.py:113): Rikai Python CallbackService is registered to SparkSession\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import rikai\n",
    "from example import spark\n",
    "\n",
    "mlflow_tracking_uri = \"sqlite:///mlruns.db\"\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "\n",
    "model = linear_model.LinearRegression()\n",
    "model.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "spark.conf.set(\"rikai.sql.ml.registry.mlflow.tracking_uri\", mlflow_tracking_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52b4c141-3894-408a-bfdd-d9648f0c2125",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'diabetes_rikai' already exists. Creating a new version of this model...\n",
      "2021/10/14 11:46:45 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: diabetes_rikai, version 3\n",
      "Created version '3' of model 'diabetes_rikai'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    rikai.mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        schema=\"float\",\n",
    "        artifact_path = \"model\",\n",
    "        registered_model_name = \"diabetes_rikai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d039f71-f5d1-4479-a078-24bb22f6e387",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-14 11:46:47,461 INFO Rikai (mlflow_registry.py:223): Resolving model diabetes_m from mlflow:///diabetes_rikai\n",
      "2021-10-14 11:46:47,489 INFO Rikai (base.py:207): Created model inference pandas_udf with name diabetes_m_ffb7cfb7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE MODEL diabetes_m USING 'mlflow:///diabetes_rikai'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35cc2686-8b96-4117-9523-dee964ee239d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|diabetes_m_ffb7cfb7(array(0.077863387626902))|\n",
      "+---------------------------------------------+\n",
      "|                                    225.97324|\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "SELECT ML_PREDICT(diabetes_m, array({diabetes_X_test[0][0]}));\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326d0f58-552f-4778-8f5a-24f76810d73b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
